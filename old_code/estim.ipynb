{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "estim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePYDxn2KIFZL",
        "colab_type": "text"
      },
      "source": [
        "#ESTIM\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOGEaopzK-H7",
        "colab_type": "text"
      },
      "source": [
        "This Python Notebook will walk you through applying different machine learning models to your own dataset (should update after writing abstract)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziHYDZ8CJDCk",
        "colab_type": "text"
      },
      "source": [
        "##Inputting Your Data\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWoQ4XGQLxiU",
        "colab_type": "text"
      },
      "source": [
        "Put all of your data in the data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq1W8jjuLR53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"/content/drive/My Drive/estim/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ3v9LdHJXq1",
        "colab_type": "text"
      },
      "source": [
        "##Importing Packages\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XkR0JEkKTyh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> General Imports:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9gnJcRCKP8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math, csv, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG0X0az4Kp5u",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Model Imports:\n",
        "\n",
        " NOTE: LIKELY MISSING SOME\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVMFz4F2Kqkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.feature_selection import chi2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX4fDRhELghO",
        "colab_type": "text"
      },
      "source": [
        "> Evaluation Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Q94OjDLlkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90jCJUkLD4Os",
        "colab_type": "text"
      },
      "source": [
        ">Mounting Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpIeRjEKB3ap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "16a255b9-92bd-4a1d-df63-25e522456065"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0sK8GgNIbwW",
        "colab_type": "text"
      },
      "source": [
        "##Intra-subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VlNkykEMGTA",
        "colab_type": "text"
      },
      "source": [
        ">MLP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzokZGfI-JQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Load data from csv file\n",
        "def loadCsv(filename):\n",
        "    lines = csv.reader(open(filename, newline= '', encoding='utf-8-sig'), delimiter=',', quotechar='|')\n",
        "    dataset = []\n",
        "    for row in lines:\n",
        "        if row[12] != \"\": # checking if the last cell in each row (column 12) is not empty\n",
        "            dataset.append([float(x) for x in row])\n",
        "    return dataset\n",
        "\n",
        "### Construct data frame\n",
        "def data(filename, scale):\n",
        "    \n",
        "    #Load data from file\n",
        "    dataset = np.array(loadCsv(filename))\n",
        "    indices_control_or_other = []\n",
        "\n",
        "    # find rows that are Control (0) or Other (9)\n",
        "    for i in range(len(dataset)):\n",
        "        if dataset[i][1] == 0 or dataset[i][1] == 9:\n",
        "            indices_control_or_other.append(i)\n",
        "    \n",
        "    dataset = np.delete(dataset, indices_control_or_other, 0) # delete rows that are Control/Other\n",
        "    dataset = np.delete(dataset,[0, 11], 1) # deleting columns 0 and 11\n",
        "\n",
        "    row_len = len(dataset[0])\n",
        "    X = np.array(dataset[:, :row_len-1])\n",
        "    y = np.array(dataset[:, row_len-1])\n",
        "    # Cap ground truth within [0, 1]\n",
        "    for i, truth in enumerate(y):\n",
        "      if truth > 1:\n",
        "        y[i] = 1\n",
        "      if truth < 0:\n",
        "        y[i] = 0\n",
        "\n",
        "    #Standardize and scale data\n",
        "    if (scale):\n",
        "        X = preprocessing.scale(X)\n",
        "    return X, y\n",
        "\n",
        "### Evaluate model\n",
        "def evaluate(model_id, X, y, scale, seed=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    index_cap = len(y_train)\n",
        "\n",
        "    all_index = []\n",
        "    index_10 = random.sample(range(index_cap),10)\n",
        "    all_index.append(index_10)\n",
        "    index_20 = random.sample(range(index_cap),20)\n",
        "    all_index.append(index_20)\n",
        "    index_50 = random.sample(range(index_cap),50)\n",
        "    all_index.append(index_50)\n",
        "    index_100 = random.sample(range(index_cap),100)\n",
        "    all_index.append(index_100)\n",
        "    index_200 = random.sample(range(index_cap),200)\n",
        "    all_index.append(index_200)\n",
        "\n",
        "    for i in range(1):\n",
        "        if (index_cap) > 500:\n",
        "            index_500 = random.sample(range(index_cap),500)\n",
        "            all_index.append(index_500)\n",
        "        else:\n",
        "            index_500 = random.sample(range(index_cap),index_cap)\n",
        "            all_index.append(index_500)\n",
        "            break\n",
        "        if (index_cap) > 1000:\n",
        "            index_1000 = random.sample(range(index_cap),1000)\n",
        "            all_index.append(index_1000)\n",
        "        else:\n",
        "            index_1000 = random.sample(range(index_cap),index_cap)\n",
        "            all_index.append(index_1000)\n",
        "            break\n",
        "        if (index_cap) > 2000:\n",
        "            index_2000 = random.sample(range(index_cap),2000)\n",
        "            all_index.append(index_2000)\n",
        "        else:\n",
        "            index_2000 = random.sample(range(index_cap),index_cap)\n",
        "            all_index.append(index_2000)\n",
        "            break\n",
        "\n",
        "  for i, indices in enumerate(all_index):\n",
        "    length = len(indices)\n",
        "    print(\"Fitting model parameters on training set\")\n",
        "    t0 = time.time()\n",
        "    grid = {'activation': ['identity','logistic', 'tanh', 'relu'], 'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "    'hidden_layer_sizes': [(10, ), (20, ), (30, ), (40, ), (50, ), (10, 10, ), (20, 10, )], }\n",
        "    clf = GridSearchCV(MLPRegressor(max_iter = 1000, solver = 'lbfgs' , random_state=seed), param_grid=grid, cv=5, iid=False, scoring='neg_mean_squared_error')\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(\"done in %0.3fs\" % (time.time() - t0))\n",
        "    print(\"\\nBest estimator found by grid search:\")\n",
        "    print('\\t'+str(clf.best_estimator_))\n",
        "\n",
        "    print(\"\\nEvaluating best estimator on test set\")\n",
        "    t0 = time.time()\n",
        "    y_pred = clf.predict(X_test)\n",
        "    # cap predictions within [0, 1]\n",
        "    for i, pred in enumerate(y_pred):\n",
        "      if pred > 1:\n",
        "        y_pred[i] = 1\n",
        "      if pred < 0:\n",
        "        y_pred[i] = 0\n",
        "    print(\"done in %0.3fs\" % (time.time() - t0))\n",
        "\n",
        "    score = round(mean_absolute_error(y_test, y_pred), 4)\n",
        "    print('\\n\\t\\tMAE (test):', score)\n",
        "    scores_MLP_intra[i].append(score)\n",
        "\n",
        "\n",
        "scores_10 = []\n",
        "scores_20 = []\n",
        "scores_50 = []\n",
        "scores_100 = []\n",
        "scores_200 = []\n",
        "scores_500 = []\n",
        "scores_1000 = []\n",
        "scores_2000 = []\n",
        "\n",
        "scores_MLP_intra = [scores_10, scores_20, scores_50, scores_100, scores_200, scores_500, scores_1000, scores_2000]\n",
        "experiment_strings = [\"10\", \"20\", \"50\", \"100\", \"200\", \"500\", \"1000\", \"2000\"]\n",
        "\n",
        "test_files = []\n",
        "\n",
        "\n",
        "#### Methods to run:\n",
        "for filepath in glob.iglob(data_dir + '/*.csv'):\n",
        "    X, y = data(filepath, True)\n",
        "    print(\"Evaluating:\" + filepath)\n",
        "    evaluate(id, X, y, True, 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34jweoQtRZ6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Plotting code for MLP intrasubject incremental\n",
        "# mean across test files\n",
        "mean_scores_MLP_intra = [np.mean(experiment_scores) for experiment_scores in scores_MLP_intra]\n",
        "\n",
        "plt.plot(experiment_strings, mean_scores_MLP_intra)\n",
        "plt.table(experiment_strings, mean_scores_MLP_intra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MO7YDdOMaLX",
        "colab_type": "text"
      },
      "source": [
        "> Random Forest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRh8ItaQMcnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZldaQxQQIoIm",
        "colab_type": "text"
      },
      "source": [
        "##Cross-Subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORFyBxyX_ekW",
        "colab_type": "text"
      },
      "source": [
        ">MLP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhoR49qV_dyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIiBR4Jk_hn6",
        "colab_type": "text"
      },
      "source": [
        "> Random Forest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atsm7PfvH69Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oOQsBhiJXY3",
        "colab_type": "text"
      },
      "source": [
        "##Incremental Pre-Trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOW4ZvS_ljS",
        "colab_type": "text"
      },
      "source": [
        ">MLP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsX7VnaI_k9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uapvZZug_mKL",
        "colab_type": "text"
      },
      "source": [
        "> Random Forest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A_48bia8TVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUB2_YsA8IQ7",
        "colab_type": "text"
      },
      "source": [
        "##Active Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP1FTFdr_1PH",
        "colab_type": "text"
      },
      "source": [
        ">MLP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5DZF3l48H8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJtu1B-_2x_",
        "colab_type": "text"
      },
      "source": [
        "> Random Forest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWvBc0gJ_2hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Active Learning Imports\n",
        "from modAL.models import ActiveLearner\n",
        "\n",
        "#### USER PARAMETERS: ####\n",
        "# Number of trials to run\n",
        "num_trials = 20\n",
        "\n",
        "# Number of rows for inital active learner training\n",
        "num_init_rows = 5\n",
        "\n",
        "# Directory with CSV files\n",
        "data_dir = \"data\"\n",
        "\n",
        "# Test ratio (for train-test split)\n",
        "test_ratio = 0.2\n",
        "#########################\n",
        "\n",
        "### Load data from csv file\n",
        "def loadCsv(filename):\n",
        "    lines = csv.reader(open(filename, newline= '', encoding='utf-8-sig'), delimiter=',', quotechar='|')\n",
        "    dataset = []\n",
        "    for row in lines:\n",
        "        if row[12] != \"\": # checking if the last cell in each row (column 12) is not empty\n",
        "            dataset.append([float(x) for x in row])\n",
        "    return dataset\n",
        "\n",
        "### Construct data frame\n",
        "def data(filename, scale):\n",
        "\n",
        "    dataset = np.array(loadCsv(filename))\n",
        "    \n",
        "    indices_control_or_other = []\n",
        "\n",
        "    # find rows that are Control (0) or Other (9)\n",
        "    for i in range(len(dataset)):\n",
        "        if dataset[i][1] == 0 or dataset[i][1] == 9:\n",
        "            indices_control_or_other.append(i)\n",
        "    \n",
        "    dataset = np.delete(dataset, indices_control_or_other, 0) # delete rows that are Control/Other\n",
        "    dataset = np.delete(dataset,[0, 11], 1) # deleting columns 0 and 11\n",
        "\n",
        "    row_len = len(dataset[0])\n",
        "    X = np.array(dataset[:, :row_len-1])\n",
        "    y = np.array(dataset[:, row_len-1])\n",
        "\n",
        "    # Cap ground truth within [0, 1]\n",
        "    for i, truth in enumerate(y):\n",
        "        if truth > 1:\n",
        "            y[i] = 1\n",
        "        if truth < 0:\n",
        "            y[i] = 0\n",
        "            \n",
        "    #Standardize and scale data\n",
        "    if (scale):\n",
        "        X = preprocessing.scale(X)\n",
        "    return X, y\n",
        "\n",
        "### RF Query Strategy (max standard deviation over tree predictions)\n",
        "def RF_std_query(active_learer, X):\n",
        "    RF = active_learer.estimator\n",
        "    per_tree_pred = np.array([tree.predict(X) for tree in RF.estimators_])\n",
        "    std_arr = np.std(per_tree_pred, axis=0) # std of predictions over all trees \n",
        "    query_idx = np.argmax(std_arr)\n",
        "    return query_idx, X[query_idx]\n",
        "\n",
        "### Evaluate model\n",
        "def evaluate(model_id, X, y, scale=False, seed=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=seed)\n",
        "\n",
        "    # choose a small amount of random rows for initial training (e.g. 5)\n",
        "    init_indices = np.random.choice(index_cap, num_init_rows, replace=False)\n",
        "    print(f\"Indices of intial train data: {init_indices}\")\n",
        "    init_X = X_train[init_indices]\n",
        "    init_y = y_train[init_indices]\n",
        "\n",
        "    # remove the initial rows from our train data\n",
        "    X_train = np.delete(X_train, init_indices, axis=0)\n",
        "    y_train = np.delete(y_train, init_indices, axis=0)\n",
        "\n",
        "    for num_train in experiments:\n",
        "        num_queries = num_train - num_init_rows\n",
        "\n",
        "        #t0 = time.time()\n",
        "        learner = ActiveLearner(estimator=RandomForestRegressor(max_features=\"auto\", criterion=\"mse\",\n",
        "         n_estimators=100, random_state=42), query_strategy=RF_std_query, X_training=init_X, y_training=init_y)\n",
        "\n",
        "        for _ in range(num_queries):\n",
        "            query_idx, query_instance = learner.query(X_train)\n",
        "            print(query_idx)\n",
        "            learner.teach(X_train[query_idx].reshape(1, -1), np.array([y_train[query_idx]]))\n",
        "\n",
        "        y_pred = learner.predict(X_test)\n",
        "        #print(\"done in %0.3fs\" % (time.time() - t0))\n",
        "\n",
        "        # cap predictions within [0, 1]\n",
        "        for i, pred in enumerate(y_pred):\n",
        "          if pred > 1:\n",
        "            y_pred[i] = 1\n",
        "          if pred < 0:\n",
        "            y_pred[i] = 0\n",
        "\n",
        "        score = round(mean_absolute_error(y_test, y_pred), 4)\n",
        "        \n",
        "        # add score for this trial to results dictionary in correct experiment row\n",
        "        results[num_train].append(score)\n",
        "\n",
        "        print(f\"MAE (test) with {num_train} rows: {score}\")\n",
        "\n",
        "def to_num(label):\n",
        "    # remove all non-numeric characters\n",
        "    label_str = \"\"\n",
        "    for char in label:\n",
        "        if char.isdigit():\n",
        "            label_str += char\n",
        "    return label_str\n",
        "\n",
        "### Main Loop (Over Files)###\n",
        "print(f\"Active Learning with Random Forest Regressor.\")\n",
        "print(f\"Model trained with {num_init_rows} rows before querying starts.\")\n",
        "\n",
        "for filepath in glob.iglob(data_dir + '/*.csv'):\n",
        "    X, y = data(filepath, True)\n",
        "    \n",
        "    index_cap = int(len(X) * (1 - test_ratio))\n",
        "\n",
        "    # num of training rows\n",
        "    experiments = [10, 20, 50, 100, 200] \n",
        "    \n",
        "    # add experiments for {500, 1000} if there are enough rows \n",
        "    if index_cap > 500:\n",
        "        experiments.append(500)\n",
        "    if index_cap > 1000:\n",
        "        experiments.append(1000)\n",
        "\n",
        "    # always run an experiment with all data\n",
        "    experiments.append(index_cap)\n",
        "\n",
        "    # results dictionary, with each key referring to a list of scores for that experiment\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    # RUN TRIALS\n",
        "    for trial in range(1, num_trials + 1):\n",
        "        print(f\"FILE {filepath}. TRIAL {trial}/{num_trials}.\")\n",
        "        evaluate(id, X, y, True, 42)\n",
        "\n",
        "    # convert to DataFrame for csv saving\n",
        "    results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "\n",
        "    results_df.to_csv(f\"RF_ActiveLearn_{to_num(filepath)}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}